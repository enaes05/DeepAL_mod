{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "git7FzaynGZ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8jtdeCDnGZ9"
      },
      "outputs": [],
      "source": [
        "# Load negative instances (non-cancer)\n",
        "df_normal=pd.read_csv('primarysite_HeadandNeckregion_sampletype_SolidTissueNormal.csv')\n",
        "df_normal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVlCkFr9nGaA"
      },
      "outputs": [],
      "source": [
        "df_normal=df_normal.T\n",
        "print('before insert label at last column: ', df_normal.shape)\n",
        "# Insert 'label' attribute with value 0\n",
        "df_normal['label']=0\n",
        "print('after insert label at last column: ', df_normal.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXj-vzsnGaC"
      },
      "outputs": [],
      "source": [
        "# Load positive instances (cancer)\n",
        "df_abnormal=pd.read_csv('primarysite_HeadandNeckregion_sampletype_PrimaryTumor.csv')\n",
        "print(df_abnormal.shape)\n",
        "df_abnormal=df_abnormal.T\n",
        "print('before insert label at last column: ', df_abnormal.shape)\n",
        "# Insert 'label' attribute with value 1\n",
        "df_abnormal['label']=1\n",
        "print('after insert label at last column: ', df_abnormal.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mvDQHsO0D1V"
      },
      "outputs": [],
      "source": [
        "# Find common attributes for both classes that\n",
        "# only have zero values\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "aux_abnormal=df_abnormal.iloc[1:,]\n",
        "aux_abnormal_cols=aux_abnormal.columns[(aux_abnormal == 0).all()]\n",
        "print(aux_abnormal_cols)\n",
        "\n",
        "aux_normal=df_normal.iloc[1:,]\n",
        "aux_normal_cols=aux_normal.columns[(aux_normal == 0).all()]\n",
        "print(aux_normal_cols)\n",
        "\n",
        "# print(np.where(aux_normal_cols=='label'))\n",
        "# Delete the 'label' column here, as it only has zero\n",
        "# values because it is the negative class label\n",
        "aux_normal_cols = aux_normal_cols.delete(18742)\n",
        "\n",
        "comun = np.intersect1d(aux_abnormal_cols, aux_normal_cols)\n",
        "print(comun, comun.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x_04yObnGaD"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4rCHJveOUxT"
      },
      "outputs": [],
      "source": [
        "# Delete the first row for both sets, it only \n",
        "# has identifiers that are not useful here\n",
        "df_normal = df_normal.iloc[1:,]\n",
        "df_normal = df_normal.drop(comun, axis=1)\n",
        "df_abnormal = df_abnormal.iloc[1:,]\n",
        "df_abnormal = df_abnormal.drop(comun, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PF-gzQtnGaG"
      },
      "outputs": [],
      "source": [
        "# Concatenate both datasets\n",
        "data = pd.concat([df_normal,df_abnormal], ignore_index=True)\n",
        "data = data.sample(n=data.shape[0], random_state=2)\n",
        "# Number of trees for Random Forest\n",
        "ntrees=100\n",
        "\n",
        "# Selecting the last column as label\n",
        "Y= data['label']\n",
        "X= data.iloc[:,:-1] \n",
        "X = np.asarray(X) \n",
        "Y = np.asarray(Y)\n",
        "\n",
        "# Training and test sets\n",
        "test_size = int(np.floor(0.30*X.shape[0]) )\n",
        "trainX, testX = X[:-test_size], X[-test_size:]\n",
        "trainY, testY = Y[:-test_size], Y[-test_size:]\n",
        "print(trainY.shape,testY.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJUb9jTWRVc9"
      },
      "outputs": [],
      "source": [
        "# Normalize train and test set\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "trainX = scaler.fit_transform(trainX)\n",
        "testX = scaler.fit_transform(testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJB84-bxnGaJ"
      },
      "outputs": [],
      "source": [
        "# Train first Random Forest\n",
        "from sklearn import preprocessing\n",
        "\n",
        "clf=RandomForestRegressor(n_estimators=ntrees, random_state=50)\n",
        "\n",
        "clf.fit(trainX,trainY)\n",
        "clf.score(testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aaJZj-6c-Wj"
      },
      "outputs": [],
      "source": [
        "# Visualize Random Forest predictions\n",
        "pred = clf.predict(testX)\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(6,6))# 6,6\n",
        "plt.figure(1) \n",
        "plt.style.use('seaborn-deep')  \n",
        "\n",
        "c=np.where(testY==0)\n",
        "print(\"test 0s number:  \",len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='darkorange', label= \"normal (%d cases in testset) \" % (len(c[0]) ))\n",
        "c=np.where(testY==1)\n",
        "print(\"test 1s number:  \", len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='blue', label= \"abnormal (%d cases in testset) \" % (len(c[0]) ))\n",
        "plt.ylabel('Numbers of events')\n",
        "plt.xlabel('Predicted score')\n",
        "plt.yscale(\"log\")\n",
        "title=\"normal versus abnormal as Primary Site\"\n",
        "plt.title(title)\n",
        "plt.legend(loc='upper center')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKN1o_7TYfJk"
      },
      "outputs": [],
      "source": [
        "indexes = (-clf.feature_importances_).argsort()[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JIezDYpaJsV"
      },
      "outputs": [],
      "source": [
        "# Keep the 5000 important attributes\n",
        "new_normal = df_normal.iloc[:, indexes]\n",
        "new_normal['label'] = 0\n",
        "\n",
        "new_abnormal = df_abnormal.iloc[:, indexes]\n",
        "new_abnormal['label'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D96MBuBfaVcC"
      },
      "outputs": [],
      "source": [
        "# Concatenate both sets again\n",
        "new_data = pd.concat([new_normal,new_abnormal], ignore_index=True)\n",
        "new_data = new_data.sample(n=new_data.shape[0],random_state=2)\n",
        "# Some parameters\n",
        "ntrees=100\n",
        "\n",
        "# Selecting the last column as label\n",
        "new_Y= new_data['label']\n",
        "new_X= new_data.iloc[:,:-1] \n",
        "new_X = np.asarray(new_X) \n",
        "new_Y = np.asarray(new_Y)\n",
        "\n",
        "# Training and test sets\n",
        "test_size = int(np.floor(0.30*new_X.shape[0]) )\n",
        "new_trainX, new_testX = new_X[:-test_size], new_X[-test_size:]\n",
        "new_trainY, new_testY = new_Y[:-test_size], new_Y[-test_size:]\n",
        "print(new_trainY.shape,new_testY.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8_J1XuLQuwZ"
      },
      "outputs": [],
      "source": [
        "# Normalize the new train/test sets\n",
        "new_trainX = scaler.fit_transform(new_trainX)\n",
        "new_testX = scaler.fit_transform(new_testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y16OXv9rcuQd"
      },
      "outputs": [],
      "source": [
        "new_clf=RandomForestRegressor(n_estimators=ntrees, random_state=8) #random_state=50\n",
        "# Train the new classifier using the reduced dataset\n",
        "new_clf.fit(new_trainX,new_trainY)\n",
        "new_clf.score(new_testX, new_testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z576JNrcRne"
      },
      "outputs": [],
      "source": [
        "# Show the new predictions\n",
        "pred = new_clf.predict(new_testX)\n",
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(6,6))# 6,6\n",
        "plt.figure(1) \n",
        "plt.style.use('seaborn-deep')  \n",
        "\n",
        "c=np.where(testY==0)\n",
        "print(\"test 0s number:  \",len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='darkorange', label= \"normal (%d cases in testset) \" % (len(c[0]) ))\n",
        "c=np.where(testY==1)\n",
        "print(\"test 1s number:  \", len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='blue', label= \"abnormal (%d cases in testset) \" % (len(c[0]) ))\n",
        "plt.ylabel('Numbers of events')\n",
        "plt.xlabel('Predicted score')\n",
        "plt.yscale(\"log\")\n",
        "title=\"normal versus abnormal as Primary Site\"\n",
        "plt.title(title)\n",
        "plt.legend(loc='upper center')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCIxSYmFeW0h"
      },
      "source": [
        "**VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rXmLHE_y74E"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers.merge import concatenate as concat\n",
        "from tensorflow.keras.layers import Lambda, Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy, BinaryFocalCrossentropy\n",
        "from tensorflow.keras.metrics import binary_focal_crossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wGlZiKEzTMb"
      },
      "outputs": [],
      "source": [
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling \n",
        "        fr an isotropic unit Gaussian.\n",
        "    # Arguments:\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns:\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    # K is the keras backend\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSnnjkKmzbWg"
      },
      "outputs": [],
      "source": [
        "# build encoder model\n",
        "inputs = Input(shape=5000, name='encoder_input')\n",
        "x = Dense(4000, activation='relu')(inputs)\n",
        "x = Dense(3000, activation='relu')(x)\n",
        "z_mean = Dense(2000, name='z_mean')(x)\n",
        "z_log_var = Dense(2000, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary \n",
        "# with the TensorFlow backend\n",
        "z = Lambda(sampling,\n",
        "           output_shape=(2000,), \n",
        "           name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUS-OUlszh4M"
      },
      "outputs": [],
      "source": [
        "# build decoder model\n",
        "latent_inputs = Input(shape=(2000,), name='z_sampling')\n",
        "x = Dense(3000, activation='relu')(latent_inputs)\n",
        "x = Dense(4000, activation='relu')(x)\n",
        "outputs = Dense(5000, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN-NF9exrIQl"
      },
      "outputs": [],
      "source": [
        "# instantiate VAE model\n",
        "\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
        "reconstruction_loss *= 10000\n",
        "# loss = BinaryFocalCrossentropy(gamma=10)\n",
        "# reconstruction_loss = loss(inputs,outputs)\n",
        "# reconstruction_loss *= 5000\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(keras.optimizers.Adam(learning_rate=0.0001))\n",
        "vae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c62YIpp0G9Z"
      },
      "outputs": [],
      "source": [
        "loss_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "# save_callback = keras.callbacks.ModelCheckpoint(filepath='training_2d.ckpt',save_best_only=True,save_weights_only=True)\n",
        "vae.fit(new_trainX, epochs=1000, batch_size=12, validation_data=(new_testX, None), callbacks=[loss_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3BLapyAvXbe"
      },
      "outputs": [],
      "source": [
        "valores = vae.predict(new_testX)\n",
        "print(\"**VALORES PREDICHOS**\")\n",
        "print(valores)\n",
        "print(\"----------------------------------------------\")\n",
        "print(\"**VALORES REALES**\")\n",
        "print(new_testX)\n",
        "print(\"----------------------------------------------\")\n",
        "pred=new_clf.predict(valores)\n",
        "print(new_clf.score(valores, new_testY))\n",
        "print(\"----------------------------------------------\")\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEDEtY6KcjPy"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(6,6))# 6,6\n",
        "plt.figure(1) \n",
        "plt.style.use('seaborn-deep')  \n",
        "\n",
        "c=np.where(new_testY==0)\n",
        "print(\"test 0s number:  \",len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='darkorange', label= \"normal (%d cases in testset) \" % (len(c[0]) ))\n",
        "c=np.where(new_testY==1)\n",
        "print(\"test 1s number:  \", len(c[0]))\n",
        "plt.hist(pred[c[0]],50,histtype='step',color='blue', label= \"abnormal (%d cases in testset) \" % (len(c[0]) ))\n",
        "plt.ylabel('Numbers of events')\n",
        "plt.xlabel('Predicted score')\n",
        "plt.yscale(\"log\")\n",
        "title=\"normal versus abnormal as Primary Site\"\n",
        "plt.title(title)\n",
        "plt.legend(loc='upper center')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh_PIkh-L3vW"
      },
      "outputs": [],
      "source": [
        "# z_mean_train, _, _ = encoder.predict(new_trainX)\n",
        "\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# zero_index = np.where(new_trainY == 0)\n",
        "# one_index = np.where(new_trainY == 1)\n",
        "# plt.scatter(z_mean_train[one_index, 0], z_mean_train[one_index, 1], c='yellow')\n",
        "# plt.scatter(z_mean_train[zero_index, 0], z_mean_train[zero_index, 1], c='purple')\n",
        "# plt.colorbar()\n",
        "# plt.xlabel(\"z[0]\")\n",
        "# plt.ylabel(\"z[1]\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSptmCfsRUAG"
      },
      "outputs": [],
      "source": [
        "# z_mean_test, _, _ = encoder.predict(new_testX)\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# zero_index = np.where(new_testY == 0)\n",
        "# one_index = np.where(new_testY == 1)\n",
        "# plt.scatter(z_mean_test[one_index, 0], z_mean_test[one_index, 1], c='yellow')\n",
        "# plt.scatter(z_mean_test[zero_index, 0], z_mean_test[zero_index, 1], c='purple')\n",
        "# plt.colorbar()\n",
        "# plt.xlabel(\"z[0]\")\n",
        "# plt.ylabel(\"z[1]\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HLyH3Hu_DfX"
      },
      "outputs": [],
      "source": [
        "# newer_trainY = np.expand_dims(new_trainY,axis=1)\n",
        "# train_set = np.concatenate((z_mean_train,newer_trainY), axis=1)\n",
        "# newer_testY = np.expand_dims(new_testY,axis=1)\n",
        "# test_set = np.concatenate((z_mean_test,newer_testY), axis=1)\n",
        "# dataset_2d = np.concatenate((train_set,test_set))\n",
        "# np.savetxt(\"dataset_2d.csv\", dataset_2d, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"dataset1.csv\", new_data, delimiter=',')\n",
        "new_data2 = scaler.fit_transform(new_data)\n",
        "np.savetxt(\"dataset2.csv\", new_data2, delimiter=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u_0AlwReWkb",
        "outputId": "6b3e166e-7a5e-4564-9908-563904b57738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z_mean_new, _, _ = encoder.predict(new_X)\n",
        "newer_Y = np.expand_dims(new_Y,axis=1)\n",
        "new_set = np.concatenate((z_mean_new,newer_Y), axis=1)\n",
        "np.savetxt(\"new_dataset.csv\", new_set, delimiter=',')\n",
        "new_set_normalized = scaler.fit_transform(new_set)\n",
        "np.savetxt(\"new_dataset_normalized.csv\", new_set_normalized, delimiter=',')"
      ],
      "metadata": {
        "id": "cQ24hj6MLx8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
